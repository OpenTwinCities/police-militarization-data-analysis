{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#!python --version    #Python 3.8.2\n",
    "#csv.__version__      #1.0\n",
    "#pd.__version__       #1.0.3\n",
    "#re.__version__       #2.2.1\n",
    "# datetime standard module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUSTOM SETTINGS: set these as appropriate for your environment\n",
    "\n",
    "# Enter the path to the local data files:\n",
    "path_datafiles = \"OriginalData/\"\n",
    "\n",
    "# Enter the path to save the clean, merged data:\n",
    "path_mergedfiles = \"MergedData/\"\n",
    "\n",
    "# This is the LESO_file from Check_DISP_AllStatesAndTerritories.ipynb\n",
    "# Please check the file there before trying this notebook/\n",
    "#LESOfile_all = \"DISP_AllStatesAndTerritories_06302020.xlsx\"\n",
    "LESOfile_all = \"DISP_AllStatesAndTerritories_03312020.xlsx\"\n",
    "# like \"DISP_Shipments_Cancellations_mmddyyyy_mmddyyyy.xlsx\"\n",
    "#LESOfile_qtr = \"DISP_Shipments_Cancellations_04012020_06302020.xlsx\"\n",
    "LESOfile_qtr = \"DISP_Shipments_Cancellations_01012020_03312020.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used by all dataframes\n",
    "ordered_columns_list = ['OriginatingFile', 'StateAbbreviation', 'RequestingAgency',\n",
    "                        'ItemDescription', 'RecordDate', 'AcquisitionValue', 'Quantity',\n",
    "                        'UnitIncrement', 'AgencyType', 'Item_FSG', 'Item_FSC', 'Item_CC',\n",
    "                        'Item_Code', 'Justification', 'NSN', 'FSC', 'NIIN', 'DEMILCode',\n",
    "                        'DEMILIC', 'StationType', 'RequisitionID' ,'CancelledBy',\n",
    "                        'RTDRef', 'ReasonCancelled']\n",
    "# used by all dataframes\n",
    "def get_agency_type(check_data):\n",
    "    agency_dictionary = {' POLICE':'police',' POL':'police',' PD':'police','POLICIA':'police',\n",
    "                         ' CONSTABLE':'police',' CSO':'community outreach',' ACADEMY':'training',\n",
    "                         ' SAFETY':'public safety',' DPS':'public safety',' PSD':'public safety',\n",
    "                         ' ENFORCEMENT':'law enforcement',' LAW ENF':'law enforcement',\n",
    "                         ' SHERIFF':'sheriff',' SHERIFF\\'S':'sheriff',' SHERIFFS':'sheriff',' SO':'sheriff',\n",
    "                         ' MARSHALL':'marshall','MARSHAL':'marshall',' PATROL':'patrol',' FIRE':'fire',\n",
    "                         ' ATTORNEY':'district attorney',' ATTY':'district attorney',' DA':'district attorney',\n",
    "                         ' PROSECUTER':'district attorney',' PROSECUTOR':'district attorney',\n",
    "                         ' NATURAL':'nat resources',' CONSERVATION':'nat resources',' PARKS':'nat resources',\n",
    "                         ' CORRECTIONS':'prison',' CORRECTION':'prison',' PRISON':'prison',\n",
    "                         ' DETENTION':'prison',' DTF':'drug',\n",
    "                         ' AGRICULTURE':'nat resources','GAME AND FISH':'nat resources','DNR ':'nat resources',\n",
    "                         'FISH AND WILDLIFE':'nat resources'}\n",
    "    for k in agency_dictionary:\n",
    "        if re.search(k,check_data):\n",
    "            return agency_dictionary[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PLEASE RUN Check_DISP_AllStatesAndTerritories.ipynb and Check_DISP_Shipments_Cancellations.ipynb FIRST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That notebook works from assumptions in the check notebooks. For more information about various columns and values:   \n",
    "https://www.dla.mil/Portals/104/Documents/DispositionServices/LESO/DISP_QuickStartGuide_11012017_hyperlinked.pdf   \n",
    "https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/ProgramFAQs.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook merges the cumulative data and the quarterly data into a single dataframe. The idea is that the original data can be created from the merged dataframe. For analysis inside a notebook, all of these columns would not be needed. This notebook can serve as a model or starting point for analyzing the data across LESO files.   \n",
    "There are notes at the end for saving the data by quarter in tab-separated files. Might want to look into using pyarrow feather format:\n",
    "https://arrow.apache.org/docs/python/feather.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns in Merged Dataframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OriginatingFile__: created from file name and sheet <br>\n",
    "> TYPE:str LENGTH: varies CHARACTER_SET: [A-Za-z0-9] and []   \n",
    "\n",
    "__StateAbbreviation__: (maps to 'State') two digit postal abbreviation <br>\n",
    "> TYPE:str LENGTH: 2 CHARACTER_SET: [A-Z]   \n",
    "\n",
    "__RequestingAgency__: (maps to 'Station Name (LEA)') where LEA stands for 'Law Enforcement Activity/Agency'<br>\n",
    "> TYPE:str LENGTH: varies CHARACTER_SET: varies   \n",
    "\n",
    "__ItemDescription__: (maps to 'Item Name') descriptive item name<br>\n",
    "> TYPE:str LENGTH: varies CHARACTER_SET: varies   \n",
    "\n",
    "__RecordDate__: (maps to 'Ship Date' AllStatesAndTerritories file)<br>\n",
    "> TYPE:datetime64 LENGTH:29 CHARACTER_SET: yyyy-mm-ddT00:00:00.000000000  \n",
    "\n",
    "__AcquisitionValue__: (maps to 'Acquisition Value') value of the requested items in dollars<br>\n",
    "> TYPE:float64 LENGTH: varies CHARACTER_SET: [0-9.]  \n",
    "\n",
    "__Quantity__: (maps to 'Quantity') number of items requested<br>\n",
    "> TYPE:int LENGTH: varies CHARACTER_SET: [0-9]   \n",
    "\n",
    " __UnitIncrement__: (maps to 'UI') unit increment<br>\n",
    "> TYPE:str LENGTH: varies CHARACTER_SET: varies   \n",
    "\n",
    "__AgencyType__: infered from 'Station Name (LEA)'<br>\n",
    "> TYPE:str LENGTH: varies CHARACTER_SET: see Expected Values, can be changed     \n",
    "\n",
    "__The following four columns are all part of the NATO Stock Number, a 13-digit alphanumeric string__<br>\n",
    "format {aa: FSG bb: FSC cc:CountryCode dddeeee: non-standard item code}<br>\n",
    "https://en.wikipedia.org/wiki/NATO_Stock_Number (NOTE: FSCG+NIIN == NSN)<br>\n",
    "__Item_FSG__: Federal Supply Group number<br>\n",
    "> TYPE:str LENGTH:2 CHARACTER_SET: alphanumeric   \n",
    "\n",
    "__Item_FSC__: Federal Supply Classification<br>\n",
    "> TYPE:str LENGTH:2 CHARACTER_SET: alphanumeric   \n",
    "\n",
    "__Item_CC__: country of origin from National Codification Bureau (aka NCB)<br>\n",
    "> TYPE:str LENGTH:2 CHARACTER_SET: alphanumeric   \n",
    "\n",
    "__Item_Code__: non-standard item code<br>\n",
    "> TYPE:str LENGTH:7 CHARACTER_SET: alphanumeric   \n",
    "\n",
    "__The following columns are found only in DISP_AllStatesAndTerritories__<br>\n",
    "__NSN__ NATO Stock Number; https://en.wikipedia.org/wiki/NATO_Stock_Number (aka: FSCG+NIIN == NSN)<br>\n",
    "> TYPE:str LENGTH:13 CHARACTER_SET: varies {aa: FSG bb: FSC cc:NCB ddd-eeee: non-standard item code}  \n",
    "\n",
    "__DEMILCode__ (maps to 'DEMIL Code' AllStatesAndTerritories only) demilitarization code<br>\n",
    "see https://www.dla.mil/HQ/LogisticsOperations/Services/FIC/DEMILCoding/DEMILCodes/<br>\n",
    "> TYPE:char LENGTH:1 CHARACTER_SET: [GPFDCEBQA]   \n",
    "\n",
    "__DEMILIC__ (maps to 'DEMIL IC' AllStatesAndTerritories only) demilitarization intgrity code<br>\n",
    "https://www.dla.mil/HQ/LogisticsOperations/Services/FIC/DEMILCoding/DEMILCodes/<br>\n",
    "> TYPE:int LENGTH: varies CHARACTER_SET: [0-9] & 'blank'(means not coded yet)   \n",
    "\n",
    "__StationType__ (maps to 'StationType' AllStatesAndTerritories only) governmental unit owns the agency<br>\n",
    "> TYPE:str LENGTH:? CHARACTER_SET: 'State'   \n",
    "\n",
    "__The following columns are found only in DISP_Shipments_Cancellations__<br>\n",
    "__FSC__: Federal Supply Classification Group number<br>\n",
    "NATO Stock Number; https://en.wikipedia.org/wiki/NATO_Stock_Number (aka: FSCG+NIIN == NSN)<br>\n",
    "> TYPE:str LENGTH:4 CHARACTER_SET: [0-9] varies {aa: FSG bb: FSC cc:NCB ddd-eeee: non-standard item code}  \n",
    "\n",
    "__NIIN__: National Item Identification number<br>\n",
    "NATO Stock Number; https://en.wikipedia.org/wiki/NATO_Stock_Number (aka: FSCG+NIIN == NSN)<br>\n",
    "> TYPE:str LENGTH:9 CHARACTER_SET: varies {aa: FSG bb: FSC cc:NCB ddd-eeee: non-standard item code}   \n",
    "\n",
    "__Justification__: (maps to 'Justification')<br>\n",
    "> TYPE:str LENGTH:? CHARACTER_SET: varies   \n",
    "\n",
    "__The following columns are found only in SHIPMENTS sheet of DISP_Shipments_Cancellations__<br>\n",
    "__RequisitionID__: (maps to 'Requisition ID' DISP_Shipments_Cancellation, sheet=SHIPMENTS) <br>\n",
    "> TYPE:str LENGTH:? CHARACTER_SET: varies   \n",
    "\n",
    "__The following columns are found only in CANCELLATIONS sheet of DISP_Shipments_Cancellations__<br>\n",
    "__CancelledBy__: (maps to 'Cancelled By' DISP_Shipments_Cancellations, sheet=CANCELLATIONS only) <br>\n",
    "> TYPE:str LENGTH: varies CHARACTER_SET: varies   \n",
    "\n",
    "__RTDRef__: (maps to 'RTD Ref')<br>\n",
    "> TYPE:str LENGTH: varies CHARACTER_SET: varies   \n",
    "\n",
    "__ReasonCancelled__: (maps to 'Reason Cancelled')<br>\n",
    "> TYPE:str LENGTH: varies CHARACTER_SET: varies   \n",
    "\n",
    "__The following column added at the end for splitting the data into quarters__<br>\n",
    "__Quarter__: created from 'RecordDate'<br>\n",
    "> TYPE: str LENGTH: 6 CHARACTER_SET: [0-9Q]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer_df created from DISP_AllStatesAndTerritories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure you have set path_datafiles and LESOfile_all variables above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_expected_columns = ['State', 'Station Name (LEA)',\n",
    "                    'NSN', 'Item Name', 'Quantity', 'UI', 'Acquisition Value',\n",
    "                    'DEMIL Code', 'DEMIL IC', 'Ship Date','Station Type']\n",
    "\n",
    "trans_columns_dictionary = {'State':'StateAbbreviation', 'Station Name (LEA)':'RequestingAgency',\n",
    "                      'NSN':'NSN', 'Item Name':'ItemDescription','Quantity':'Quantity',\n",
    "                      'UI':'UnitIncrement', 'Acquisition Value':'AcquisitionValue',\n",
    "                      'DEMIL Code':'DEMILCode','DEMIL IC':'DEMILIC',\n",
    "                      'Ship Date':'RecordDate', 'Station Type':'StationType'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all sheets in the spreadsheet are read into a dictionary of dataframes\n",
    "# see Check_DISP_AllStatesAndTerritoriesipynb for a full explanation\n",
    "excel_dict = pd.read_excel(\"file:\" + path_datafiles + LESOfile_all, sheet_name=None,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141068"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CHECK What is the number of records in the original data?\n",
    "total_transfers = 0\n",
    "for k in excel_dict:\n",
    "    total_transfers = total_transfers + len(excel_dict[k])\n",
    "total_transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a single dataframe with the records for all states/territories.\n",
    "#    rename the columns to new schema (see 'Columns in Merged Dataframe' above)\n",
    "#    strip leading/trailing white space from object types\n",
    "transfer_df = pd.concat(\n",
    "    [pd.concat([v],ignore_index=True) for k,v in excel_dict.items()],ignore_index=True).\\\n",
    "    rename(columns=trans_columns_dictionary).\\\n",
    "    apply(lambda x: x.str.strip() if x.dtype == 'object' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done with excel_dict, so release a bit of memory.\n",
    "excel_dict.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best guess for the AgencyType based on RequestingAgency; if no guess use 'other' as place holder.\n",
    "transfer_df['AgencyType'] = transfer_df.RequestingAgency.map(get_agency_type)\n",
    "transfer_df['AgencyType'].fillna('other',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break 'NSN' into NATO Stock Number units. (see 'Columns in Merged Dataframe' above)\n",
    "transfer_df = transfer_df.assign(Item_FSG=transfer_df['NSN'].str.replace('-','').str[:2].values,\n",
    "                   Item_FSC=transfer_df['NSN'].str.replace('-','').str[2:4].values,\n",
    "                   Item_CC=transfer_df['NSN'].str.replace('-','').str[4:6].values,\n",
    "                   Item_Code=transfer_df['NSN'].str.replace('-','').str[6:].values,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing columns with 'not in file' value to distinguish them from NaN/null values.\n",
    "transfer_df['FSC'],transfer_df['NIIN'] = 'not in file','not in file'\n",
    "transfer_df['Justification'] = 'not in file'\n",
    "transfer_df['RequisitionID'],transfer_df['CancelledBy'] = 'not in file','not in file'\n",
    "transfer_df['RTDRef'],transfer_df['ReasonCancelled'] = 'not in file','not in file'\n",
    "# Fill 'OriginatingFile' column.\n",
    "transfer_df['OriginatingFile'] = LESOfile_all + '[ALL]'\n",
    "# TODO original sheet in file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the columns in preparation for merging.\n",
    "transfer_df = transfer_df[ordered_columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141068, 24)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_df.shape\n",
    "# Only DEMILIC should have NaN/null values; (see 'Columns in Merged Dataframe' above)\n",
    "#transfer_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shipments_df created from DISP_Shipments_Cancellations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure you have set path_datafiles and LESOfile_qtr variables above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_expected_columns = ['State', 'Station Name (LEA)', 'Requisition ID', 'FSC', 'NIIN',\n",
    "                    'Item Name', 'UI', 'Quantity', 'Acquisition Value', 'Date Shipped',\n",
    "                    'Justification']\n",
    "ship_columns_dictionary = {'State':'StateAbbreviation', 'Station Name (LEA)':'RequestingAgency',\n",
    "                      'Requisition ID':'RequisitionID', 'FSC':'FSC', 'NIIN':'NIIN',\n",
    "                      'Item Name':'ItemDescription', 'UI':'UnitIncrement', 'Quantity':'Quantity',\n",
    "                      'Acquisition Value':'AcquisitionValue', 'Date Shipped':'RecordDate', \n",
    "                      'Justification':'Justification'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 'SHIPMENTS' sheet in the original file is read into a dataframe.\n",
    "# see Check_DISP_Shipments_Cancellations.ipynb for a full explanation\n",
    "#    rename the columns to new schema (see 'Columns in Merged Dataframe' above)\n",
    "#    strip leading/trailing white space from object types\n",
    "shipments_df = pd.read_excel(\"file:\" + path_datafiles + LESOfile_qtr, sheet_name='SHIPMENTS').\\\n",
    "                             rename(columns=ship_columns_dictionary).\\\n",
    "                             apply(lambda x: x.str.strip() if x.dtype == 'object' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6353, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CHECK What is the number of records in the original data?\n",
    "shipments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best guess for the AgencyType based on RequestingAgency; if no guess use 'other' as place holder.\n",
    "shipments_df['AgencyType'] = shipments_df.RequestingAgency.map(get_agency_type)\n",
    "shipments_df['AgencyType'].fillna('other',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break 'FSC' and 'NIIN' into NATO Stock Number units. (see 'Columns in Merged Dataframe' above)\n",
    "shipments_df = shipments_df.assign(Item_FSG=shipments_df['FSC'].astype(str).str[:2],\n",
    "                   Item_FSC=shipments_df['FSC'].astype(str).str[2:4],\n",
    "                   Item_CC=shipments_df['NIIN'].str[:2].values,\n",
    "                   Item_Code=shipments_df['NIIN'].str[2:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing columns with 'not in file' value to distinguish them from NaN/null values.\n",
    "shipments_df['NSN'],shipments_df['DEMILCode'] = 'not in file','not in file'\n",
    "shipments_df['DEMILIC'],shipments_df['StationType'] = 'not in file','not in file'\n",
    "shipments_df['CancelledBy'],shipments_df['RTDRef'] = 'not in file','not in file'\n",
    "shipments_df['ReasonCancelled'] = 'not in file'\n",
    "# Fill 'OriginatingFile' column.\n",
    "shipments_df['OriginatingFile'] = LESOfile_qtr + '[SHIPMENTS]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the columns in preparation for merging.\n",
    "shipments_df = shipments_df[ordered_columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6353, 24)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shipments_df.shape\n",
    "# Expect no NaN/null values; (see 'Columns in Merged Dataframe' above)\n",
    "#shipments_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare cancellations_df From DISP_Shipments_Cancellations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure you have set path_datafiles and LESOfile_qtr variables above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "canc_expected_columns = ['Cancelled By', 'RTD Ref', 'State', 'Station Name (LEA)',\n",
    "                         'FSC', 'NIIN', 'Item Name', 'UI', 'Quantity', 'Acquisition Value',\n",
    "                         'Date Requested', 'Justification', 'Reason Cancelled']\n",
    "canc_columns_dictionary = {'Cancelled By':'CancelledBy', 'RTD Ref':'RTDRef', \n",
    "                           'State':'StateAbbreviation', 'Station Name (LEA)':'RequestingAgency',\n",
    "                           'FSC':'FSC', 'NIIN':'NIIN', 'Item Name':'ItemDescription',\n",
    "                           'UI':'UnitIncrement', 'Quantity':'Quantity', 'Acquisition Value':'AcquisitionValue',\n",
    "                           'Date Requested':'RecordDate', 'Justification':'Justification',\n",
    "                           'Reason Cancelled':'ReasonCancelled'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 'CANCELLATIONS' sheet in the original file is read into a dataframe.\n",
    "# see Check_DISP_Shipments_Cancellations for a full explanation\n",
    "#    rename the columns to new schema (see 'Columns in Merged Dataframe' above)\n",
    "#    strip leading/trailing white space from object types\n",
    "cancellations_df = pd.read_excel(\"file:\" + path_datafiles + LESOfile_qtr, sheet_name='CANCELLATIONS').\\\n",
    "                             rename(columns=canc_columns_dictionary).\\\n",
    "                             apply(lambda x: x.str.strip() if x.dtype == 'object' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best guess for the AgencyType based on RequestingAgency; if no guess use 'other' as place holder.\n",
    "cancellations_df['AgencyType'] = cancellations_df.RequestingAgency.map(get_agency_type)\n",
    "cancellations_df['AgencyType'].fillna('other',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break 'FSC' and 'NIIN' into NATO Stock Number units. (see 'Columns in Merged Dataframe' above)\n",
    "cancellations_df = cancellations_df.assign(Item_FSG=cancellations_df['FSC'].astype(str).str[:2],\n",
    "                   Item_FSC=cancellations_df['FSC'].astype(str).str[2:4],\n",
    "                   Item_CC=cancellations_df['NIIN'].str[:2].values,\n",
    "                   Item_Code=cancellations_df['NIIN'].str[2:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing columns with 'not in file' value to distinguish them from NaN/null values.\n",
    "cancellations_df['NSN'],cancellations_df['DEMILCode'] = 'not in file','not in file'\n",
    "cancellations_df['DEMILIC'],cancellations_df['StationType'] = 'not in file','not in file'\n",
    "cancellations_df['RequisitionID'] = 'not in file'\n",
    "# Fill 'OriginatingFile' column.\n",
    "cancellations_df['OriginatingFile'] = LESOfile_qtr + '[CANCELLATIONS]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the columns in preparation for merging.\n",
    "cancellations_df = cancellations_df[ordered_columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6640, 24)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancellations_df.shape\n",
    "# Found NaN/null values in 'Justification' and 'ReasonCancelled'; (see 'Columns in Merged Dataframe' above)\n",
    "#cancellations_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if list(transfer_df.columns) != list(shipments_df.columns):\n",
    "    print('Columns do not match.')\n",
    "elif list(transfer_df.columns) != list(cancellations_df.columns):\n",
    "    print('Columns do not match.')\n",
    "elif list(shipments_df.columns) != list(cancellations_df.columns):\n",
    "    print('Columns do not match.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df = pd.concat([transfer_df,shipments_df,cancellations_df],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts on Storing the Merged Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION: which columns are categorical?   \n",
    "NOTE: none of these are evenly distributed   \n",
    "OriginatingFile: categories will grow each month, so dates will change off these three files   \n",
    ">DISP_AllStatesAndTerritories_mmddyyyy.xlsx[ALL]   \n",
    ">DISP_Shipments_Territories_mmddyyyy_mmddyyyy.xlsx[SHIPMENTS]   \n",
    ">DISP_Shipments_Territories_mmddyyyy_mmddyyyy.xlsx[CANCELLATIONS]   \n",
    "\n",
    "StateAbbreviation: categorized by state; there are 59 possibilities based on   \n",
    ">US Postal Service Publication 28 https://pe.usps.com/text/pub28/28apb.htm   \n",
    ">(see Check_DISP_AllStatesAndTerritories.ipynb and Check_DISP_Shipments_Cancellations.ipynb   \n",
    "\n",
    "Item_FSG: categorized by federal supply group; there are 100 possibilites, not evenly distributed   \n",
    ">see https://en.wikipedia.org/wiki/Federal_Stock_Number#External_links   \n",
    ">and https://en.wikipedia.org/wiki/List_of_NATO_Supply_Classification_Groups to start tracking these down   \n",
    "\n",
    "AgencyType: rough guess of types of agencies based on station names, currently 13 categories   \n",
    "\n",
    "TO EXPLORE: feather/parquet? should we save with more catgories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to save the entire dataset, this breaks the data into quarters\n",
    "# and saves it in tab-separated files by quarter.\n",
    "# BEWARE: so far no check on whether the data in files overlaps\n",
    "all_data_df['Quarter'] = pd.PeriodIndex(all_data_df.RecordDate, freq='Q')                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the first time running it:\n",
    "for i in list(all_data_df['Quarter'].unique()):\n",
    "    #print(type(all_data_df[all_data_df['Quarter'] == i]))\n",
    "    all_data_df[all_data_df['Quarter'] == i].to_csv('MergedData\\\\LESO_' + str(i) + '.tsv', \n",
    "                                                    index=False, mode='w', sep='\\t', escapechar=\"\\\\\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If appending to existing data:\n",
    "for i in list(all_data_df['Quarter'].unique()):\n",
    "    #print(type(all_data_df[all_data_df['Quarter'] == i]))\n",
    "    all_data_df[all_data_df['Quarter'] == i].to_csv('MergedData\\\\LESO_' + str(i) + '.tsv', header=False,\n",
    "                                                    index=False, mode='a', sep='\\t', escapechar=\"\\\\\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read the data file:\n",
    "#df = pd.read_csv('MergedData\\\\LESO_2020Q1.tsv',sep='\\t',header=[0],index_col=None,\n",
    "#                 quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")\n",
    "\n",
    "#df.columns\n",
    "#df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
