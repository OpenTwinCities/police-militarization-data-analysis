{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#!python --version     #Python 3.8.5\n",
    "#pd.__version__       #1.1.2\n",
    "#re.__version__       #2.2.1\n",
    "# datetime standard module\n",
    "# pathlib standard module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUSTOM SETTINGS: set these as appropriate for your environment\n",
    "\n",
    "# Enter the path to the local data files:\n",
    "path_datafiles = \"../../data/\"\n",
    "\n",
    "# Enter the path to save the clean, merged data:\n",
    "path_mergedfiles = \"../../data/merged/\"\n",
    "\n",
    "# This is the LESO_file from Check_DISP_AllStatesAndTerritories.ipynb\n",
    "# Please check the file there before trying this notebook/\n",
    "#LESOfile_all = \"DISP_AllStatesAndTerritories_03312020.xlsx\"\n",
    "#LESOfile_all = \"DISP_AllStatesAndTerritories_06302020.xlsx\"\n",
    "#LESOfile_all = \"DISP_AllStatesAndTerritories_09302020.xlsx\"\n",
    "# like \"DISP_Shipments_Cancellations_mmddyyyy_mmddyyyy.xlsx\"\n",
    "#LESOfile_qtr = \"DISP_Shipments_Cancellations_01012020_03312020.xlsx\"\n",
    "#LESOfile_qtr = \"DISP_Shipments_Cancellations_04012020_06302020.xlsx\"\n",
    "#LESOfile_qtr = \"DISP_Shipments_Cancellations_07012020_09302020.xlsx\"\n",
    "\n",
    "# values for OriginatingFiles column\n",
    "originate_allstates = LESOfile_all + '[ALL]'\n",
    "originate_shipments = LESOfile_qtr + '[SHIPMENTS]'\n",
    "originate_cancellations = LESOfile_qtr + '[CANCELLATIONS]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used by all dataframes\n",
    "ordered_columns_list = ['OriginatingFile', 'StateAbbreviation', 'RequestingAgency',\n",
    "                        'ItemDescription', 'RecordDate', 'AcquisitionValue', 'Quantity',\n",
    "                        'UnitIncrement', 'Item_FSG', 'Item_FSC', 'Item_CC',\n",
    "                        'Item_Code', 'Justification', 'NSN', 'FSC', 'NIIN', 'DEMILCode',\n",
    "                        'DEMILIC', 'StationType', 'RequisitionID' ,'CancelledBy',\n",
    "                        'RTDRef', 'ReasonCancelled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PLEASE RUN Check_DISP_AllStatesAndTerritories.ipynb and Check_DISP_Shipments_Cancellations.ipynb FIRST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That notebook works from assumptions in the check notebooks. For more information about various columns and values:   \n",
    "https://www.dla.mil/Portals/104/Documents/DispositionServices/LESO/DISP_QuickStartGuide_11012017_hyperlinked.pdf   \n",
    "https://www.dla.mil/DispositionServices/Offers/Reutilization/LawEnforcement/ProgramFAQs.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook merges the AllStatesAndTerrirtories and Shipments_Cancellations data into a single dataframe. When you run the notebook, you can decide how to save the data (examples for all in one tsv file or split into files by quarter or state abbreviation).\n",
    "\n",
    "The idea is that the original data can be recreated from the merged dataframe. For analysis inside a notebook, all of these columns would not be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Dictionary for Merged Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "| Field | Data Type | Description | Original Column | Length | Expected Pattern | null? |   \n",
    "| ----- | ---- | ---- | ---- | ---- |---- | ---- |   \n",
    "||| __Constructed Fields__ |||||   \n",
    "| OriginatingFile | string | file that populated this record | created from LESO filename and sheet | varies | see Custom Settings above | no |   \n",
    "| Item_FSG | string | supply category the item belongs to; see [Federal Supply Group Number](https://en.wikipedia.org/wiki/List_of_NATO_Supply_Classification_Groups#References) | file dependent, digits 1&2 of \\['NSN','FSC'\\]| 2 | \\[0-9\\]{2} | no |   \n",
    "| Item_FSC | string | supply class the item belongs to; see [Federal Supply Group Number](https://en.wikipedia.org/wiki/List_of_NATO_Supply_Classification_Groups#References) | file dependent, digits 3&4 of \\['NSN','FSC'\\] | 2 | \\[0-9\\]{2} | no |   \n",
    "| Item_CC | string | country code for where final assembly of item occurred (a.k.a. nation code; see [Federal Supply Group Number](https://en.wikipedia.org/wiki/National_Codification_Bureau) | file dependent, digits 5&6 of \\['NSN'\\] or digits 1&2 of \\['NIIN'\\]| 2 | \\[0-9\\]{2} | no |   \n",
    "| Item_Code | string | supply class the item belongs to; see [Federal Supply Group Number](https://en.wikipedia.org/wiki/List_of_NATO_Supply_Classification_Groups#References) | file dependent, last 7 digits of \\['NSN','NIIN'\\] | 7 | \\[0-9\\]{7} | no |   \n",
    "||| __Fields in All Files__ |||||   \n",
    "| StateAbbreviation | string | two digit postal abbreviation for U.S. state or territory | State | 2 | \\[A-Z\\]\\[A-Z\\] | no |   \n",
    "| RequestingAgency | string | descriptive name of requesting law enforcement agency | Station Name (LEA) | varies | varies | no |   \n",
    "| ItemDescription | string | descriptive name of requested item | Item Name | varies | varies | no |   \n",
    "| RecordDate | datetime64 | date | file dependent \\['Ship Date','Date Shipped','Date Requested'\\] | 29 | yyyy-mm-ddT00:00:00.000000000 | no |   \n",
    "| AcquisitionValue | float | U.S. dollar amount paid when the item was originally purchased by the government | Acquisition Value | varies | [0-9]+.[0-9]{2} | no |   \n",
    "| Quantity | integer | number of units requested | Quantity | varies | [0-9]+ | no |   \n",
    "| UnitIncrement | string | units of requested item known as unit increments | UI | varies | varies | no |   \n",
    "||| __Fields in AllStatesAndTerritories Only__ | __fill value 'not in file'__ ||||   \n",
    "| NSN | string | [NATO Stock Number](https://en.wikipedia.org/wiki/NATO_Stock_Number) a government-assigned identifier for requested item | NSN | 9 | \\[0-9\\]{4}-\\[0-9\\]{2}-\\[A-Z0-9\\]{3}-\\[A-Z0-9\\]{4} | no |   \n",
    "| DEMILCode | character | [demilitarization code](https://www.dla.mil/HQ/LogisticsOperations/Services/FIC/DEMILCoding/DEMILCodes/) for level of destruction required when the item leaves Department of Defense control | DEMIL Code | 1 | \\[GPFDCEBQA\\] | no |   \n",
    "| DEMILIC | integer | [demilitarization itegrity code](https://www.dla.mil/HQ/LogisticsOperations/Services/FIC/DEMILCoding/DEMILCodes/) validity of DEMIL Code (a missing value means it has not yet been reviewed), see [FLIS manual](https://www.dla.mil/HQ/LogisticsOperations/TrainingandReference/FLISProcedures/) for more information | DEMIL IC | 1 | [0-9] or blank | yes |   \n",
    "| StationType | string | level of government associated with requesting agency; needs further research | Station Type | 5 | 'State' | no |   \n",
    "||| __Fields in Shipments_Cancellations Only__ | __fill value 'not in file'__ ||||   \n",
    "| FSC | string | [Federal Supply Number](https://en.wikipedia.org/wiki/NATO_Stock_Number#Federal_Supply_Classification_Group_(FSCG)) consisting of the Federal Supply Group and Federal Supply Classification | FSC | 4 | \\[0-9\\]{4} | no |   \n",
    "| NIIN | string | [National Item Identification Number](https://en.wikipedia.org/wiki/NATO_Stock_Number#National_Item_Identification_Number_(NIIN)) a Country Code followed by a 7-digit item identifier string | NIIN | 9 | \\[0-9\\]{9} | no |   \n",
    "| Justification | string | descriptive text justifying request; needs further research | Justification | varies | varies | yes |   \n",
    "||| __Fields in Shipments Only__ | __fill value 'not in file'__ ||||   \n",
    "| RequisitionID | string | apparently unique identifier needs further research | Requisition ID | 14 | [A-z0-9]{14} | no |   \n",
    "||| __Fields in Cancellations Only__ | __fill value 'not in file'__ ||||   \n",
    "| CancelledBy | string | apparently agency that cancelled request; needs further research | Cancelled By | varies | varies | yes | \n",
    "| RTDRef | string | apparently unique identifier; needs further research | RTD Ref | 6 or 7 | [0-9]{7} | no |     \n",
    "| ReasonCancelled | string | why request is cancelled; needs further research | Reason Cancelled | varies | varies | yes |   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer_df created from DISP_AllStatesAndTerritories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure you have set path_datafiles and LESOfile_all variables above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_expected_columns = ['State', 'Station Name (LEA)',\n",
    "                    'NSN', 'Item Name', 'Quantity', 'UI', 'Acquisition Value',\n",
    "                    'DEMIL Code', 'DEMIL IC', 'Ship Date','Station Type']\n",
    "\n",
    "trans_columns_dictionary = {'State':'StateAbbreviation', 'Station Name (LEA)':'RequestingAgency',\n",
    "                      'NSN':'NSN', 'Item Name':'ItemDescription','Quantity':'Quantity',\n",
    "                      'UI':'UnitIncrement', 'Acquisition Value':'AcquisitionValue',\n",
    "                      'DEMIL Code':'DEMILCode','DEMIL IC':'DEMILIC',\n",
    "                      'Ship Date':'RecordDate', 'Station Type':'StationType'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all sheets in the spreadsheet are read into a dictionary of dataframes\n",
    "# see Check_DISP_AllStatesAndTerritoriesipynb for a full explanation\n",
    "excel_dict = pd.read_excel(\"file:\" + path_datafiles + LESOfile_all, sheet_name=None,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK What is the number of records in the original data?\n",
    "total_transfers = 0\n",
    "for k in excel_dict:\n",
    "    total_transfers = total_transfers + len(excel_dict[k])\n",
    "total_transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a single dataframe with the records for all states/territories.\n",
    "#    rename the columns to new schema (see 'Columns in Merged Dataframe' above)\n",
    "#    strip leading/trailing white space from object types\n",
    "transfer_df = pd.concat(\n",
    "    [pd.concat([v],ignore_index=True) for k,v in excel_dict.items()],ignore_index=True).\\\n",
    "    rename(columns=trans_columns_dictionary).\\\n",
    "    apply(lambda x: x.str.strip() if x.dtype == 'object' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done with excel_dict, so release a bit of memory.\n",
    "excel_dict.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break 'NSN' into NATO Stock Number units. (see 'Columns in Merged Dataframe' above)\n",
    "transfer_df = transfer_df.assign(Item_FSG=transfer_df['NSN'].str.replace('-','').str[:2].values,\n",
    "                   Item_FSC=transfer_df['NSN'].str.replace('-','').str[2:4].values,\n",
    "                   Item_CC=transfer_df['NSN'].str.replace('-','').str[4:6].values,\n",
    "                   Item_Code=transfer_df['NSN'].str.replace('-','').str[6:].values,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing columns with 'not in file' value to distinguish them from NaN/null values.\n",
    "transfer_df['FSC'],transfer_df['NIIN'] = 'not in file','not in file'\n",
    "transfer_df['Justification'] = 'not in file'\n",
    "transfer_df['RequisitionID'],transfer_df['CancelledBy'] = 'not in file','not in file'\n",
    "transfer_df['RTDRef'],transfer_df['ReasonCancelled'] = 'not in file','not in file'\n",
    "# Fill 'OriginatingFile' column.\n",
    "transfer_df['OriginatingFile'] = originate_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the columns in preparation for merging.\n",
    "transfer_df = transfer_df[ordered_columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_df.shape\n",
    "# Only DEMILIC should have NaN/null values; (see 'Columns in Merged Dataframe' above)\n",
    "#transfer_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shipments_df created from DISP_Shipments_Cancellations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure you have set path_datafiles and LESOfile_qtr variables above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_expected_columns = ['State', 'Station Name (LEA)', 'Requisition ID', 'FSC', 'NIIN',\n",
    "                    'Item Name', 'UI', 'Quantity', 'Acquisition Value', 'Date Shipped',\n",
    "                    'Justification']\n",
    "ship_columns_dictionary = {'State':'StateAbbreviation', 'Station Name (LEA)':'RequestingAgency',\n",
    "                      'Requisition ID':'RequisitionID', 'FSC':'FSC', 'NIIN':'NIIN',\n",
    "                      'Item Name':'ItemDescription', 'UI':'UnitIncrement', 'Quantity':'Quantity',\n",
    "                      'Acquisition Value':'AcquisitionValue', 'Date Shipped':'RecordDate', \n",
    "                      'Justification':'Justification'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 'SHIPMENTS' sheet in the original file is read into a dataframe.\n",
    "# see Check_DISP_Shipments_Cancellations.ipynb for a full explanation\n",
    "#    rename the columns to new schema (see 'Columns in Merged Dataframe' above)\n",
    "#    strip leading/trailing white space from object types\n",
    "shipments_df = pd.read_excel(\"file:\" + path_datafiles + LESOfile_qtr, sheet_name='SHIPMENTS').\\\n",
    "                             rename(columns=ship_columns_dictionary).\\\n",
    "                             apply(lambda x: x.str.strip() if x.dtype == 'object' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK What is the number of records in the original data?\n",
    "shipments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break 'FSC' and 'NIIN' into NATO Stock Number units. (see 'Columns in Merged Dataframe' above)\n",
    "shipments_df = shipments_df.assign(Item_FSG=shipments_df['FSC'].astype(str).str[:2],\n",
    "                   Item_FSC=shipments_df['FSC'].astype(str).str[2:4],\n",
    "                   Item_CC=shipments_df['NIIN'].str[:2].values,\n",
    "                   Item_Code=shipments_df['NIIN'].str[2:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing columns with 'not in file' value to distinguish them from NaN/null values.\n",
    "shipments_df['NSN'],shipments_df['DEMILCode'] = 'not in file','not in file'\n",
    "shipments_df['DEMILIC'],shipments_df['StationType'] = 'not in file','not in file'\n",
    "shipments_df['CancelledBy'],shipments_df['RTDRef'] = 'not in file','not in file'\n",
    "shipments_df['ReasonCancelled'] = 'not in file'\n",
    "# Fill 'OriginatingFile' column.\n",
    "shipments_df['OriginatingFile'] = originate_shipments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the columns in preparation for merging.\n",
    "shipments_df = shipments_df[ordered_columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shipments_df.shape\n",
    "# Expect no NaN/null values; (see 'Columns in Merged Dataframe' above)\n",
    "#shipments_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare cancellations_df From DISP_Shipments_Cancellations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure you have set path_datafiles and LESOfile_qtr variables above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canc_expected_columns = ['Cancelled By', 'RTD Ref', 'State', 'Station Name (LEA)',\n",
    "                         'FSC', 'NIIN', 'Item Name', 'UI', 'Quantity', 'Acquisition Value',\n",
    "                         'Date Requested', 'Justification', 'Reason Cancelled']\n",
    "canc_columns_dictionary = {'Cancelled By':'CancelledBy', 'RTD Ref':'RTDRef', \n",
    "                           'State':'StateAbbreviation', 'Station Name (LEA)':'RequestingAgency',\n",
    "                           'FSC':'FSC', 'NIIN':'NIIN', 'Item Name':'ItemDescription',\n",
    "                           'UI':'UnitIncrement', 'Quantity':'Quantity', 'Acquisition Value':'AcquisitionValue',\n",
    "                           'Date Requested':'RecordDate', 'Justification':'Justification',\n",
    "                           'Reason Cancelled':'ReasonCancelled'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 'CANCELLATIONS' sheet in the original file is read into a dataframe.\n",
    "# see Check_DISP_Shipments_Cancellations for a full explanation\n",
    "#    rename the columns to new schema (see 'Columns in Merged Dataframe' above)\n",
    "#    strip leading/trailing white space from object types\n",
    "cancellations_df = pd.read_excel(\"file:\" + path_datafiles + LESOfile_qtr, sheet_name='CANCELLATIONS').\\\n",
    "                             rename(columns=canc_columns_dictionary).\\\n",
    "                             apply(lambda x: x.str.strip() if x.dtype == 'object' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break 'FSC' and 'NIIN' into NATO Stock Number units. (see 'Columns in Merged Dataframe' above)\n",
    "cancellations_df = cancellations_df.assign(Item_FSG=cancellations_df['FSC'].astype(str).str[:2],\n",
    "                   Item_FSC=cancellations_df['FSC'].astype(str).str[2:4],\n",
    "                   Item_CC=cancellations_df['NIIN'].str[:2].values,\n",
    "                   Item_Code=cancellations_df['NIIN'].str[2:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing columns with 'not in file' value to distinguish them from NaN/null values.\n",
    "cancellations_df['NSN'],cancellations_df['DEMILCode'] = 'not in file','not in file'\n",
    "cancellations_df['DEMILIC'],cancellations_df['StationType'] = 'not in file','not in file'\n",
    "cancellations_df['RequisitionID'] = 'not in file'\n",
    "# Fill 'OriginatingFile' column.\n",
    "cancellations_df['OriginatingFile'] = originate_cancellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the columns in preparation for merging.\n",
    "cancellations_df = cancellations_df[ordered_columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancellations_df.shape\n",
    "# Found NaN/null values in 'Justification' and 'ReasonCancelled'; (see 'Columns in Merged Dataframe' above)\n",
    "#cancellations_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO per Nicole...more information for user; explain this better\n",
    "# https://sandimetz.com/speaking OOP coding principles\n",
    "\n",
    "if list(transfer_df.columns) != list(shipments_df.columns):\n",
    "    print('Columns do not match.')\n",
    "elif list(transfer_df.columns) != list(cancellations_df.columns):\n",
    "    print('Columns do not match.')\n",
    "elif list(shipments_df.columns) != list(cancellations_df.columns):\n",
    "    print('Columns do not match.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df = pd.concat([transfer_df,shipments_df,cancellations_df],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to break the merged dataframe into files\n",
    "# by state abbreviation\n",
    "save_on_column = 'StateAbbreviation'\n",
    "\n",
    "# If you want to break the merged dataframe into quarters\n",
    "# and save it in tab-separated files by quarter.\n",
    "#all_data_df['Quarter'] = pd.PeriodIndex(all_data_df.RecordDate, freq='Q')\n",
    "#save_on_column = 'Quarter'\n",
    "\n",
    "for i in list(all_data_df[save_on_column].unique()):\n",
    "    my_file = Path(path_mergedfiles + 'LESO_' + str(i) + '.tsv')\n",
    "    if my_file.exists():\n",
    "        all_data_df[all_data_df[save_on_column] == i].to_csv(my_file, header=False,\n",
    "                                                    index=False, mode='a', sep='\\t', escapechar=\"\\\\\")\n",
    "    else:\n",
    "        all_data_df[all_data_df[save_on_column] == i].to_csv(my_file, \n",
    "                                                    index=False, mode='w', sep='\\t', escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want the whole dataframe in one file\n",
    "#my_file = Path(path_mergedfiles + 'LESO_mergeall.tsv')\n",
    "#if my_file.exists():\n",
    "#    all_data_df.to_csv(my_file, header=False, index=False, mode='a', sep='\\t', escapechar=\"\\\\\")\n",
    "#else:\n",
    "#    all_data_df.to_csv(my_file, index=False, mode='w', sep='\\t', escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to read the data file into a dataframe:\n",
    "#my_file = Path(path_mergedfiles + filename)\n",
    "#df = pd.read_csv(my_file,sep='\\t',header=[0],index_col=None,\n",
    "#                 quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
